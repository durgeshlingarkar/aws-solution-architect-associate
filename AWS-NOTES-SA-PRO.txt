SA-PRO

Domain 8 - Cloud Migration & Hybrid

- VCenter has a pluggin that enables you to migrate VMware VMs to Amazon EC2 and manage AWS resources from within VCenter.
- Usecases for this are 
	- Migrate VMWare VMs to Amazon EC2
	- Reach new geographies from VCenter
	- Self service AWS portal within VCenter
	- Leverage VCenter experience while getting started with AWS.
	
- Use Storage gateway to migrate your existing VMs to AWS
- To ensure snapshots are consistent, power down your VM and then take the snapshot in offline mode.

- Data Pipeline
	- Data pipeline is a webservice that helps you to process or move data between different AWS compute and storage services.
	- Can be ingerated with on premises environment
	- Can be scheduled
	- Data pipeline will provision and terminate resources as and when required. 
	- A lot of data pipeline's functionality is replaced by LAMBDA
	- A data pipeline consists of 
		- Datanode : where you are gonna store the data.
		- Activity : Any activity to process/move data
		- Precondition : Conditions or filter to trigger activity
		- Schedule : Schedule for activity to run

VPC & CIDR 

- When you create a subnet, you create a CIDR block for the subnet. The allowed block size is between /28 netmask and /16 netmask. 
- If you create more than one subnet in a VPC, the CIDR blocks of the subnets must not overlap.
- 5 IP addressed are always reserved per CIDR block. Following are the reseved IP addresses for CIDR block 10.0.0.0/24
	- 10.0.0.0 - Network address
	- 10.0.0.1 - Reserved by AWS for VPC router.
	- 10.0.0.2 - Reserved by AWS for mapping to Amazon provided DNS.
	- 10.0.0.3 - Reserved by AWS for future use.
	- 10.0.0.255 - Network broadcast address. AWS does not support broadcast in a VPC, therefore they reserve this address.
	
	

VPN to Direct Connect Migrations. 

- Most customers will have site-to-site VPN tunnel from their location to AWS. But as traffic gets heavier, they will opt to use direct connect.
- Once direct connect is installed, you should configure it so that your VPN connection & your direct connect connection are within the same BGP community. You then configure BGP, so that your VPN connection has a higher cost than the Direct connect connection. 


Networking Components

Elastic Network Interfaces

- An elastic network interface (referred to as a network interface in this documentation) is a virtual network interface that can include the following attributes:

	- a primary private IPv4 address
	- one or more secondary private IPv4 addresses
	- one Elastic IP address per private IPv4 address
	- One public IPv4 address, which can be auto-assigned to the network interface for eth0 when you launch an instance
	- one or more IPv6 addresses
	- one or more security groups
	- a MAC address
	- a source/destination check flag
	- a description
	
- You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance.
- Each instance in your VPC has a default network interface (the primary network interface) that is assigned a private IPv4 address from the IPv4 address range of your VPC. You cannot detach a primary network interface from an instance.
- You can create and attach an additional network interface to any instance in your VPC. The number of network interfaces you can attach varies by instance type.


Domain 7 : Scalability & Elasticity

Cloudfront 

- is used to deliver entire website, including dynamic, static, streaming and interactive content using a global network of edge locations. 
- Requests for contents are automaticallu routed to the nearest edge location, so content is delivered with best possible performance. Cloudfront is optimized to work with with other webservices like S3, EC2, ELB & Route53. 
- 2 types of distributions 
	- Web Distribution - Streams HTTP protocol
	- RTMP - Streams RTMP protocol
- Geo restriction also called as Geo blocking lets you choose the countries where you want to restrict access to your content. 
- Whitelists or Blacklists particular countries using either API or console. 
- Viewers from blacklisted countries will see HTTP 403 error. You can also create custom error pages. 
- Cloudfront supports GET, HEAD, POST, PUT, PATCH, DELETE and OPTIONS.
- It does not cache responses to POST, PUT, PATCH, DELETE requests, these requests are proxied back to origin server. 
- You can use both HTTP & HTTPS with cloudfront. 
- With SSL, you can use the default cloudfront URL or ur custom URL with your own certificate.
- For custom URL with your own certificates there are 2 ways 
	- Dedicated IP custom SSL : Allocated dedicated IP addresses to serve your SSL content to each cloudfront edge location. Very expensive. $600 USD per certificate per month. However, will support older browsers. 
	- SNI custom SSL : Relies on SNI (Server Name Indication) extenstion of transport layer security protocol, which allows multiple domains to serve SSL traffic over  the same IP address by including the hostname, viewers are trying to connect to. Older browsers won't support it. 
- CNAME : You can have 100 CNAMES aliases to each distribution. Cloudfront supports wildcard CNAMES. 
- Invalidations Requests : There are multuiple options for removing a file from the edge locations. You can simple delete the file from your origin and as content in the edge locations reaches the expiration period defined in the each object's HTTP header, it will be removed. If there is offensive or potentially harmful material that needs to be removed before the expiration then you can use Invalidation API to remove the object from all cloudfront edge locations. 
- Zone APEX Support : Using Route53, AWS's authoritative DNS service, you can configure an 'Alias' record that lets you map the apex or root (example.com) of your DNS name to your AMAZON Cloudfront distribution. Eg http://example.com --> http://d12878rt.cloudfront.net . Route53 will then respond to each request with the right IP addresses for your Cloudfront distribution. Route53 doesn't charge for queries to Alias records that are mapped to a cloudfront distribution. 
- Dynamic content support : Cloudfront supports delivery of dynamic content that is customized or personalized using HTTP cookies. To use this feature, you specify whether you want cloudfront to forward some or all cookies to ur custom origin server. cloudfront that considers the forwarded cookie values when identifying a unique object in it's cache. This way you end users get both the benefit of content that is personalized just for them with a cookie and the performance benefits of cloudfront. You can optionally choose to log the cookie values in cloudfront access logs. 
- Cost Saving with Amazon CloudFront
	-Amazon CloudFront has no minimum commitments and charges you only for what you use. 
	- Compared to self-hosting, Amazon CloudFront spares you from the expense and complexity of operating a network of cache servers in multiple sites across the internet and eliminates the need to over-provision capacity in order to serve potential spikes in traffic.
	-Amazon CloudFront also uses techniques such as collapsing simultaneous viewer requests at an edge location for the same file into a single request to your origin server. This reduces the load on your origin servers reducing the need to scale your origin infrastructure, which can bring you further cost savings.
	- if you are using an AWS origin (e.g., Amazon S3, Amazon EC2, etc.), effective December 1, 2014, we are no longer charging for AWS data transfer out to Amazon CloudFront. This applies to data transfer from all AWS regions to all global CloudFront edge locations.
- CloudFront supports origin redundancy
	- For every origin that you add to a CloudFront distribution, you can assign a backup origin that can used to automatically serve your traffic if the primary origin is unavailable
	- You can choose a combination of HTTP 4xx/5xx status codes that, when returned from the primary origin, trigger the failover to the backup origin. The two origins can be any combination of AWS and non-AWS origins.
- CloudFront Regional Edge Cache
	- CloudFront delivers your content through a worldwide network of data centers called edge locations. The regional edge caches are located between your origin web server and the global edge locations that serve content directly to your viewers. This helps improve performance for your viewers while lowering the operational burden and cost of scaling your origin resources.
- Amazon CloudFront establishes WebSocket connections only when the client includes the 'Upgrade: websocket' header and the server responds with the HTTP status code 101 confirming that it can switch to the WebSocket protocol.
- Field-Level Encryption is a feature of CloudFront that allows you to securely upload user-submitted data such as credit card numbers to your origin servers. Using this functionality, you can further encrypt sensitive data in an HTTPS form using field-specific encryption keys (which you supply) before a PUT/ POST request is forwarded to your origin. 
- Usecase for field level encryption 
	- Many web applications collect sensitive data such as credit card numbers from users that is then processed by application services running on the origin infrastructure. All these web applications use SSL/TLS encryption between the end user and CloudFront, and between CloudFront and your origin. Now, your origin could have multiple micro-services that perform critical operations based on user input. However, typically sensitive information only needs to be used by a small subset of these micro-services, which means most components have direct access to these data for no reason. A simple programming mistake, such as logging the wrong variable could lead to a customer’s credit card number being written to a file.With field-level encryption, CloudFront’s edge locations can encrypt the credit card data. From that point on, only applications that have the private keys can decrypt the sensitive fields. So the order fulfillment service can only view encrypted credit card numbers, but the payment services can decrypt credit card data. This ensures a higher level of security since even if one of the application services leaks cipher text, the data remains cryptographically protected.
- Amazon CloudFront has an optional private content feature. When this option is enabled, Amazon CloudFront will only deliver files when you say it is okay to do so by securely signing your requests. 


Elasticache 
	- Memchached
	- Use Memcached if 
		- You want the simplest model possible. 
		- You need to run large nodes with multiple cores or threads. 
		- You need the ability to scale out, adding and removing nodes as demand on your system increases or decreases. 
		- You want to shard your data across multiple nodes. 
		- You need to cache objects, such as a database.
		
	- Redis
	- Use Redis if
		- You want complex data types such as strings, hashes, lists and sets. 
		- You want to sort or rank in-memory data-sets. 
		- You want persistence of your key store. 
		- You want to replicate your data from the primary to one or more read replicas for availability. 
		- You need automatic failover if any one of your primary nodes fail.
		- You want publish and subscribe (pub/sub) capabilities - the client being informed of events on the server.
		- You want backup and restore capabilities.
		
Kinesis Streams
	- Kinesis stream enables you to build custom applications that process or analyze streaming data for specialized needs.You can continously add various types of data, such as clickstreams, application logs, and social media to an Amazon Kinesis Stream from hundreds of thousands of sources. Within seconds, the data will be available for your Amazon Kinesis applications to read and process from the stream.
	- Key concepts
		- Data producers can produce data using
			- Amazon Kinesis Streams API
				- PutRecord (Single data record)
				- PutRecords (Multiple data record)
			- Amazon Kinesis Producer Library (KPL)
				- On GitHub. By using KPL, customers do not need to develope the same logic every time they create a new application for data ingestion.
			- Amazon Kinesis Agent
				- Prebuilt java application you can install on your linux devices. 
		- Shards
			- A shard is simply the unit of measurement of data when reffering to Kinesis. One shard provides a capacity of 1MB/Sec data input and 2MB/Sec data output. One shard can support upto 1000 PUT records per second. You will specify the number of shards needed when you create a stream. For example, you can create a stream with 2 shards. This stream has a throughput of 2MB/sec data input and 4MB/sec data output, and allows upto 2000 PUT records per second. You can dynamically add or remove. shards from your stream as your data throughput changes via resharding.
		- Records
			- Sequence Number
				- Each data record has a unique sequence number. Think of it as a unique key. The sequence number is assigned by streams after you write to the stream with client.PutRecord or client.PutRecords .You cant use sequence number to logically seperate data in terms of what shards they have come from - you can only do this using partition keys.
			- Partition Key
				- Kinesis streams can consist of many different shards. You can group the data by shard by using a partition key. Essentinally, the partion key tells you which shard the data belongs to. A partition key is specified by the applications putting the data into the stream. 
			- Data Itself(Blob)
				- Data blobs are the data your producer adds to the stream. The maximum size of a data blob (the data payload after base-64 decoding) is 1MB.
		- Data Consumers (Kinesis Streams Applications)
		- Data is stored for 24 hours by defaut within streams. This can be increased upto 7 days.
		- Use S3, Redshift etc to store processed data for longer term. Kinesis streams is not persistent storage.
		

Domain 6 - Security 
	- Active Directory Services
		- AD Connector (Used for existing AD Deployments)
			- Great way to connect into existing AD environments.
			- Supports MFA
		- Simple AD (Used for new deployments)
			- MFA not supported
			- Cannot add additional AD servers
			- No trust relationships. 
			- Cannot transfer FSMO roles
			
	- STS
		- Grants users limited and temporary access to AWS resources. Users can come from 3 sources. 
			- Federation (Typically Active Directory)
				- Uses Security Assertion Markup Language (SAML)
				- Grants temporary access based off the users Active Directory credentials. Does not need to be a user in IAM
				- Single signon allows users to login to AWS console without assigning IAM credentials. 
			- Federation with Mobile Apps
				- Uses Facebook/Amazon/Google or other OpenId providers to login.
			- Cross Account Access
				- Lets users from account access resources in other account. 
		
		- Keys Terms 
			- Federation : Combining or joining a list of users in one domain (such as IAM) with a list of users in another domain (Such as Active Directory, Facebook etc)
			- Identity Broker : A service (software) that allows you to take an identity from point A and joint it to point B (federate it)
			- Identity Store : Services like Active Directory, Facebook, Google etc. 
			- Identities : A user of services like Facebook etc.
		
		- STS Scenario 
			- Employees enter their username and password. 
			- The application calls an Identity broker. The broker captures the username and password. 
			- The Identity broker users the organization's LDAP directory to validate the employee's identity. 
			- The Identity broker calls the new GetFederationToken function using IAM credentials. The call must include an IAM policy and a duration (1 to 36 hours) along with a policy that specifies the permission to be granted to the temporary security credentials. 
			- The Security Token Service confirms that the policy of the IAM user making the call to GetFederationToken gives permissions to create new tokens and then returns 4 values to the application. An access key, a secret access key, a token and a duration (token's TTL)
			- The Identity broker then returns the temporary security credentials to the reporting application. 
			- The data storage application uses the temporary security credentials (including the token) to make requests to Amazon S3. 
			- Amazon S3 uses IAM to verify that the credentials allow the requested operation on the given S3 bucket and key.
			- IAM provides S3 with the go-ahead to perform the requested operation. 
			
	
	- Monitoring in Cloud
		- CloudTrail
			- It enables you to retrieve the history of API calls and other events for all the regions in your account. This includes calls and events made by AWS Management Console and Command Line tools, by any other AWS SDK, or by other AWS services.
			- CloudTrail is used for Auditing and collecting a history of API calls made on your environment. It is not a logging service per se. 
			
		
		- CloudWatch
			- It is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use amazon cloudwatch to collect and track metrics, collect and monitor log files, and set alarms. 
			- Cloudwatch can monitor AWS resources such as Amazon EC2 instances, DynamoDB tables, RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. 
			- By default, CloudWatch logs will store your log data indefinitely. You can change the retention for each log group at any time. 
			- Cloudwatch alarm history is only stored for 14 days. 
			
		- Cloudwatch to monitor cloudtrail
			- You can monitor cloudtrail in real time is by sending the cloudtrail logs to cloudwatch logs. 
			- For a trail that is enabled in all regions in your account, CloudTrail send log files from all those regions to a CloudWatch Logs log group. 
			- You can define cloudwatch logs metric filters that will evaluate your cloudtrail log events for matches in terms, phrases, or values. You assign cloudwatch metrics to metrics filters. You can also create cloudwatch alarms that are triggered according to thresholds and time periods that you specify.
			
		- You can monitor events and ship these logs to 
			- Cloudwatch
			- A centralized logging system (AlertLogic, Splunk, Sumo logic)
			- S3 (via a script that exports the log file)
		- Avoid storing your logs anywhere that's not persistent. Like
			- Root device volume on Ec2 instances. 
			- Ephemeral storage. 
		- Best places to store your logs are usually 
			- S3 
			- Cloudwatch
			
		- CloudTrail can be used across multiple accounts in a single S3 bucket. (You can dump your cloudtrail logs from all accounts into that one bucket)
		
		- Cloudwatch supports Cross Account Subscriptions (you can monitor multiple accounts in a single cloudwatch)
		
	- Cloud Hardware Security Modules (HSM)
		- HSM stands for Hardware Security Module, and is a physical device that safegaurds and manages digital keys for strong authentication and provides crypto processing. These modules traditionally come in the form of a plug-in card or an external device that attaches directly to a computer or a network server.
		- HSM is single tenanted (1 physical device, for you only)
		- Must be used within a VPC
		- You can use VPC peering to connect to HSM
		- You can use EBS volume encryption, S3 Object encryption, and key management with HSM, but this does require custom application scripting. 
		- If you need fault tolerance, you need to build a cluster, so you will need 2. If you have only 1 and if it fails, you will lose your keys.
		- Can integrate with RDS as well as Redshift. 
		- You can monitor HSM via Syslog.
		
	- DDOS (Distributed Denial of Service)
		- It is an attack that attempts to make your website or application unavailable to your end users. 
		- This can be achieved by multiuple mechanisms like 
			- Large packet floods.
			- By using combination of refelction and amplification techniques.
				- Amplification/Reflection attacks can include things such as NTP, SSDP, DNS, Chargen, SNMP attacks, etc. and is where an attacker may send a third party server (such as a NTP server) a request using a spoofed IP address. The server will then respond to that request with a greater payload than initial request (usually 28 * 54 times larger than the request) to the spoofed IP address. This means that if an attacker sends a packet with a spoofed IP address of 64 bytes, the NTP server would respond with up to 3456 bytes of traffic. Attacker can co ordinate this and use multiple NTP servers a second to send legitimate NTP traffic to the target. 
			- By using large botnets.
			
		- How to mitigate DDOS 
			- Minimize the attack surface area.
				- Some production environmnets has multiple entry points into them. Perhaps they allow directy SSH/RDP access to their web/application/DB servers for management. 
				- This can be minimized by using a bastion/jump box that only allows access to specific white listed IP addresses to these bastion servers and move the web/application/DB servers into private subnets. 
				- By minimizing the attack surface area, you are limiting your exposure to just a few hardened entry points. 
				
			- Be Ready to scale to absorb the attack.
				- The Key strategy behind DDOS attack is to bring your infrastructure to a breaking point. This strategy assumes one thing: that you can't scale to meet the attack. 
				- The easiest way to defeat this strategy is to design your infrastructure to scale as and when it is needed. You can scale both horizontly or vertically. 
				- Scaling has following benefits 
					- The attack is spread over larger area. 
					- Attackers then have to counter attack, taking up more of their resources. 
					- Scaling buys you time to analyze the attack and to respond with appropriate counter measures. 
					- Scaling has the added benefit of providing you with additional levels of redundancy. 
					
			- Safegaurd exposed resources.
				- In situation where you cannot eliminate internet entry points to your applications, you will need to take additional measures to restrict access and protect those entry points without interrupting legitimate end user traffic. 
				- Three resources that can provide this control and flexibility are Cloudfront, Route53 and WAFs.
					- Cloudfront 
						- Geo Restriction/Blocking : Restrict access to users in specific countries (Using whitelists or blacklists)
						- Origin Access Identity : Restrict access to your S3 bucket so that people can only access S3 using Cloudfront URLs.
					- Route53 
						- Alias Record Sets : You can use these immediately redirect your traffic to an amazon cloudfront distribution, or to a different ELB with higher capacity EC2 servers running WAFs or your own security tools. No DNS change and no need to worry about propagation. 
						- Private DNS : Allows you to manage internal DNS names for your application resources (web servers, application servers, database servers etc) without exposing this information to public internet.
						
					
			- Learn normal behaviour.
				- Be aware of normal and unsual behaviour. Know the different types of traffic and what normal levels of this traffic should be. 
				- Understand expected and unexpected resource spikes. 
				- Knowing normal behaviour allows you to spot abnormalities fast.
				- You can create alarms to alert you of normal behaviour. 
				- Helps you to create forensic data to understand the attack. 
				
			- Create a plan for attacks.
				- Having a plan ensures you have validated the design of your architecture. 
				- You understand the costs for your increased resiliency and already what techniques to employ when you come under attack.
				- You know who to contact when an attack happens. 
				
			
			
		- 
